{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next Word Prediction using LSTM_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXpryUR9B4fz",
        "outputId": "8efab711-b2a8-4b77-f763-e48686a9e1a8"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import re\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD4JwPINGdmO"
      },
      "source": [
        "training_doc3 = \"How are you? How many days since we last met? How are your parents?\""
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpVP3-OYYPE8"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRFMoML1FcR6",
        "outputId": "e0196f47-b354-4156-f037-bb26b12f75bf"
      },
      "source": [
        "cleaned = re.sub(r'\\W+', ' ', training_doc3).lower()\n",
        "tokens = word_tokenize(cleaned)\n",
        "print(len(tokens))\n",
        "#print(tokens)\n",
        "# Next we'll create sequences taking 4 cosequtive tokens at a time\n",
        "train_len = 4\n",
        "text_sequences = []\n",
        "pos = 0\n",
        "items = train_len\n",
        "for i in range(train_len,len(tokens)+1):\n",
        "  text_sequences.append(tokens[pos:items])\n",
        "  pos+=1\n",
        "  items+=1\n",
        "print(text_sequences)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "[['how', 'are', 'you', 'how'], ['are', 'you', 'how', 'many'], ['you', 'how', 'many', 'days'], ['how', 'many', 'days', 'since'], ['many', 'days', 'since', 'we'], ['days', 'since', 'we', 'last'], ['since', 'we', 'last', 'met'], ['we', 'last', 'met', 'how'], ['last', 'met', 'how', 'are'], ['met', 'how', 'are', 'your'], ['how', 'are', 'your', 'parents']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJBGzIa_GwWi",
        "outputId": "e9de55ab-9493-472e-d354-001f224b2c44"
      },
      "source": [
        "# let's create sequences with the obtained integer values for each unique token\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "print(sequences)\n",
        "print(tokenizer.word_index) # to check how the unique tokens have been mapped in our sequences"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 9, 1], [2, 9, 1, 3], [9, 1, 3, 4], [1, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7], [5, 6, 7, 8], [6, 7, 8, 1], [7, 8, 1, 2], [8, 1, 2, 10], [1, 2, 10, 11]]\n",
            "{'how': 1, 'are': 2, 'many': 3, 'days': 4, 'since': 5, 'we': 6, 'last': 7, 'met': 8, 'you': 9, 'your': 10, 'parents': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djTk--mKSrPt",
        "outputId": "41c15926-e767-436d-fe28-a7ca6c8a1876"
      },
      "source": [
        "target = []\n",
        "sequence_list = []\n",
        "for i in sequences:\n",
        "  #print(i)\n",
        "  sequence_list.append(i[:-1])\n",
        "  target.append(i[-1])\n",
        "  \n",
        "print(sequences)\n",
        "print(sequence_list,target)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 9, 1], [2, 9, 1, 3], [9, 1, 3, 4], [1, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7], [5, 6, 7, 8], [6, 7, 8, 1], [7, 8, 1, 2], [8, 1, 2, 10], [1, 2, 10, 11]]\n",
            "[[1, 2, 9], [2, 9, 1], [9, 1, 3], [1, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8], [7, 8, 1], [8, 1, 2], [1, 2, 10]] [1, 3, 4, 5, 6, 7, 8, 1, 2, 10, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHYIbdyRYJBJ"
      },
      "source": [
        "## Crate Training inputs & Training labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-6MziiNM6HE",
        "outputId": "13a120ee-3d3a-4b12-9ed8-b5fa20910627"
      },
      "source": [
        "ohe = OneHotEncoder(sparse = False)\n",
        "targets = np.array(target).reshape(-1,1)\n",
        "targets = ohe.fit_transform(targets)\n",
        "train_targets = np.concatenate((np.zeros((targets.shape[0],1)),targets),axis = 1) # adding a zero to match target\n",
        "print(train_targets)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0_Tif8qHKYa",
        "outputId": "b7203cbd-e2f0-4577-b9ed-bf8cbcdb0bcb"
      },
      "source": [
        "train_inputs = np.array(sequence_list)\n",
        "print(train_inputs)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  2  9]\n",
            " [ 2  9  1]\n",
            " [ 9  1  3]\n",
            " [ 1  3  4]\n",
            " [ 3  4  5]\n",
            " [ 4  5  6]\n",
            " [ 5  6  7]\n",
            " [ 6  7  8]\n",
            " [ 7  8  1]\n",
            " [ 8  1  2]\n",
            " [ 1  2 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7yNgupX2NC"
      },
      "source": [
        "## Build & Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2-zFf44Qsa6",
        "outputId": "3f0e4696-b59a-4dee-e998-de9987066c55"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
        "\n",
        "vocabulary_size = len(tokenizer.word_index)\n",
        "embedding_dim = 6 #train_inputs.shape[0]\n",
        "max_length = train_inputs.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocabulary_size, output_dim = embedding_dim, input_length = max_length))\n",
        "model.add(LSTM(50,return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(50,activation='relu'))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "# compiling the network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(train_inputs,train_targets,epochs=120,verbose=1)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 3, 6)              66        \n",
            "_________________________________________________________________\n",
            "lstm_34 (LSTM)               (None, 3, 50)             11400     \n",
            "_________________________________________________________________\n",
            "lstm_35 (LSTM)               (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 11)                561       \n",
            "=================================================================\n",
            "Total params: 34,777\n",
            "Trainable params: 34,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/120\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3979 - accuracy: 0.0000e+00\n",
            "Epoch 2/120\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3970 - accuracy: 0.0909\n",
            "Epoch 3/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3962 - accuracy: 0.1818\n",
            "Epoch 4/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3953 - accuracy: 0.1818\n",
            "Epoch 5/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.3945 - accuracy: 0.1818\n",
            "Epoch 6/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3936 - accuracy: 0.1818\n",
            "Epoch 7/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3926 - accuracy: 0.1818\n",
            "Epoch 8/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3916 - accuracy: 0.1818\n",
            "Epoch 9/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3905 - accuracy: 0.1818\n",
            "Epoch 10/120\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3894 - accuracy: 0.1818\n",
            "Epoch 11/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3881 - accuracy: 0.1818\n",
            "Epoch 12/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3869 - accuracy: 0.1818\n",
            "Epoch 13/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3856 - accuracy: 0.1818\n",
            "Epoch 14/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3842 - accuracy: 0.1818\n",
            "Epoch 15/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.3828 - accuracy: 0.1818\n",
            "Epoch 16/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3812 - accuracy: 0.1818\n",
            "Epoch 17/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3796 - accuracy: 0.1818\n",
            "Epoch 18/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3778 - accuracy: 0.1818\n",
            "Epoch 19/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3759 - accuracy: 0.1818\n",
            "Epoch 20/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3739 - accuracy: 0.1818\n",
            "Epoch 21/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3718 - accuracy: 0.1818\n",
            "Epoch 22/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3694 - accuracy: 0.1818\n",
            "Epoch 23/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3669 - accuracy: 0.1818\n",
            "Epoch 24/120\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3642 - accuracy: 0.1818\n",
            "Epoch 25/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3612 - accuracy: 0.1818\n",
            "Epoch 26/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3581 - accuracy: 0.1818\n",
            "Epoch 27/120\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3547 - accuracy: 0.1818\n",
            "Epoch 28/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3511 - accuracy: 0.1818\n",
            "Epoch 29/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3473 - accuracy: 0.1818\n",
            "Epoch 30/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3431 - accuracy: 0.1818\n",
            "Epoch 31/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3387 - accuracy: 0.1818\n",
            "Epoch 32/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.3339 - accuracy: 0.1818\n",
            "Epoch 33/120\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3288 - accuracy: 0.1818\n",
            "Epoch 34/120\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3234 - accuracy: 0.1818\n",
            "Epoch 35/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3175 - accuracy: 0.1818\n",
            "Epoch 36/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3113 - accuracy: 0.1818\n",
            "Epoch 37/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3047 - accuracy: 0.1818\n",
            "Epoch 38/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2976 - accuracy: 0.1818\n",
            "Epoch 39/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2901 - accuracy: 0.1818\n",
            "Epoch 40/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.2821 - accuracy: 0.1818\n",
            "Epoch 41/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2737 - accuracy: 0.1818\n",
            "Epoch 42/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2649 - accuracy: 0.1818\n",
            "Epoch 43/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2556 - accuracy: 0.1818\n",
            "Epoch 44/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2458 - accuracy: 0.1818\n",
            "Epoch 45/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2355 - accuracy: 0.1818\n",
            "Epoch 46/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2246 - accuracy: 0.1818\n",
            "Epoch 47/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2131 - accuracy: 0.1818\n",
            "Epoch 48/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.2009 - accuracy: 0.1818\n",
            "Epoch 49/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1877 - accuracy: 0.1818\n",
            "Epoch 50/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1735 - accuracy: 0.1818\n",
            "Epoch 51/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1581 - accuracy: 0.1818\n",
            "Epoch 52/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1413 - accuracy: 0.1818\n",
            "Epoch 53/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1229 - accuracy: 0.1818\n",
            "Epoch 54/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1028 - accuracy: 0.1818\n",
            "Epoch 55/120\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.0813 - accuracy: 0.1818\n",
            "Epoch 56/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0585 - accuracy: 0.2727\n",
            "Epoch 57/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0335 - accuracy: 0.2727\n",
            "Epoch 58/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0066 - accuracy: 0.2727\n",
            "Epoch 59/120\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9779 - accuracy: 0.2727\n",
            "Epoch 60/120\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9471 - accuracy: 0.2727\n",
            "Epoch 61/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9147 - accuracy: 0.3636\n",
            "Epoch 62/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8810 - accuracy: 0.3636\n",
            "Epoch 63/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8454 - accuracy: 0.3636\n",
            "Epoch 64/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8087 - accuracy: 0.3636\n",
            "Epoch 65/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7711 - accuracy: 0.2727\n",
            "Epoch 66/120\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7321 - accuracy: 0.2727\n",
            "Epoch 67/120\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.6923 - accuracy: 0.3636\n",
            "Epoch 68/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6536 - accuracy: 0.3636\n",
            "Epoch 69/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6143 - accuracy: 0.3636\n",
            "Epoch 70/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5745 - accuracy: 0.3636\n",
            "Epoch 71/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5347 - accuracy: 0.4545\n",
            "Epoch 72/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4952 - accuracy: 0.4545\n",
            "Epoch 73/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4557 - accuracy: 0.4545\n",
            "Epoch 74/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.4155 - accuracy: 0.4545\n",
            "Epoch 75/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3746 - accuracy: 0.4545\n",
            "Epoch 76/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3335 - accuracy: 0.4545\n",
            "Epoch 77/120\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2923 - accuracy: 0.4545\n",
            "Epoch 78/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2518 - accuracy: 0.3636\n",
            "Epoch 79/120\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2124 - accuracy: 0.4545\n",
            "Epoch 80/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1740 - accuracy: 0.5455\n",
            "Epoch 81/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1362 - accuracy: 0.5455\n",
            "Epoch 82/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0999 - accuracy: 0.5455\n",
            "Epoch 83/120\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0662 - accuracy: 0.5455\n",
            "Epoch 84/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0336 - accuracy: 0.5455\n",
            "Epoch 85/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0017 - accuracy: 0.5455\n",
            "Epoch 86/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9705 - accuracy: 0.5455\n",
            "Epoch 87/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9394 - accuracy: 0.5455\n",
            "Epoch 88/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9091 - accuracy: 0.5455\n",
            "Epoch 89/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8808 - accuracy: 0.6364\n",
            "Epoch 90/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8541 - accuracy: 0.7273\n",
            "Epoch 91/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8283 - accuracy: 0.7273\n",
            "Epoch 92/120\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.8049 - accuracy: 0.8182\n",
            "Epoch 93/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7820 - accuracy: 0.8182\n",
            "Epoch 94/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7588 - accuracy: 0.8182\n",
            "Epoch 95/120\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7365 - accuracy: 0.8182\n",
            "Epoch 96/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7151 - accuracy: 0.8182\n",
            "Epoch 97/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6930 - accuracy: 0.8182\n",
            "Epoch 98/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6707 - accuracy: 0.8182\n",
            "Epoch 99/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6494 - accuracy: 0.8182\n",
            "Epoch 100/120\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6280 - accuracy: 0.8182\n",
            "Epoch 101/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6063 - accuracy: 0.8182\n",
            "Epoch 102/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5842 - accuracy: 0.8182\n",
            "Epoch 103/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5626 - accuracy: 0.8182\n",
            "Epoch 104/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5415 - accuracy: 0.8182\n",
            "Epoch 105/120\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5206 - accuracy: 0.8182\n",
            "Epoch 106/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4998 - accuracy: 0.8182\n",
            "Epoch 107/120\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4794 - accuracy: 0.9091\n",
            "Epoch 108/120\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4597 - accuracy: 0.9091\n",
            "Epoch 109/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4406 - accuracy: 0.9091\n",
            "Epoch 110/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4218 - accuracy: 0.9091\n",
            "Epoch 111/120\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4037 - accuracy: 0.9091\n",
            "Epoch 112/120\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3857 - accuracy: 1.0000\n",
            "Epoch 113/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3686 - accuracy: 1.0000\n",
            "Epoch 114/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3521 - accuracy: 1.0000\n",
            "Epoch 115/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3358 - accuracy: 1.0000\n",
            "Epoch 116/120\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3203 - accuracy: 1.0000\n",
            "Epoch 117/120\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3052 - accuracy: 1.0000\n",
            "Epoch 118/120\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2907 - accuracy: 1.0000\n",
            "Epoch 119/120\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2768 - accuracy: 1.0000\n",
            "Epoch 120/120\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2635 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1c-33tiACoL2",
        "outputId": "926f4800-d623-4a49-e309-5a3d67998c7f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2557448590>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91vU63CTXXJM"
      },
      "source": [
        "## Predicting the next word (3 most probable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxdW5PXDO9iq",
        "outputId": "deee9041-f489-4344-baa5-66f5bc4146a1"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "input_text = input().strip().lower()\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=max_length, truncating='pre')\n",
        "print(encoded_text, pad_encoded)\n",
        "for i in (model.predict(pad_encoded)[0].argsort()[::-1][:3]):\n",
        "  pred_word = tokenizer.index_word[i]\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how many days\n",
            "[1, 3, 4] [[1 3 4]]\n",
            "Next word suggestion: since\n",
            "Next word suggestion: many\n",
            "Next word suggestion: how\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}